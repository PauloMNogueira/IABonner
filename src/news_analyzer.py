"""News Analyzer
M√≥dulo para an√°lise de not√≠cias usando Ollama como provedor principal com sistema de fallback
"""

import os
import json
import logging
import requests
from typing import List, Dict, Optional
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None
from dotenv import load_dotenv

class NewsAnalyzer:
    """Analisador de not√≠cias usando Ollama como provedor principal com fallback"""
    
    def __init__(self):
        load_dotenv()
        self.logger = logging.getLogger(__name__)
        
        # Configura√ß√µes dos provedores em ordem de prioridade (Ollama primeiro)
        self.providers = {
            'ollama': {
                'api_key': 'local',  # Ollama n√£o precisa de API key
                'base_url': os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434/v1'),
                'model': os.getenv('OLLAMA_MODEL', 'llama3:latest'),
                'free': True,
                'priority': 1
            },
            'openrouter': {
                'api_key': os.getenv('OPENROUTER_API_KEY'),
                'base_url': 'https://openrouter.ai/api/v1',
                'model': os.getenv('DEFAULT_MODEL', 'meta-llama/llama-3.3-70b-instruct:free'),
                'free': True,
                'priority': 2
            },
            'groq': {
                'api_key': os.getenv('GROQ_API_KEY'),
                'base_url': 'https://api.groq.com/openai/v1',
                'model': os.getenv('GROQ_MODEL', 'llama3-8b-8192'),
                'free': True,
                'priority': 3
            },
            'together': {
                'api_key': os.getenv('TOGETHER_API_KEY'),
                'base_url': 'https://api.together.xyz/v1',
                'model': os.getenv('TOGETHER_MODEL', 'meta-llama/Llama-2-7b-chat-hf'),
                'free': False,  # $1 cr√©dito inicial
                'priority': 4
            },
            'huggingface': {
                'api_key': os.getenv('HUGGINGFACE_API_KEY'),
                'base_url': 'https://api-inference.huggingface.co/models',
                'model': os.getenv('HUGGINGFACE_MODEL', 'microsoft/DialoGPT-medium'),
                'free': True,
                'priority': 5
            }
        }
        
        # Tentar configurar provedores em ordem de prioridade
        self.available_providers = []
        self._setup_providers()
        
        self.analysis_enabled = os.getenv('ANALYSIS_ENABLED', 'true').lower() == 'true'
        self.current_provider_index = 0
    
    def _check_ollama_availability(self):
        """Verificar se Ollama est√° rodando e dispon√≠vel"""
        try:
            ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
            # Remover /v1 se estiver presente para o health check
            health_url = ollama_url.replace('/v1', '') + '/api/tags'
            
            response = requests.get(health_url, timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                if models:
                    self.logger.info(f"ü¶ô Ollama dispon√≠vel com {len(models)} modelos")
                    return True
                else:
                    self.logger.warning("ü¶ô Ollama rodando mas sem modelos instalados")
                    return False
            return False
        except Exception as e:
            self.logger.debug(f"ü¶ô Ollama n√£o dispon√≠vel: {e}")
            return False
    
    def _setup_providers(self):
        """Configurar provedores dispon√≠veis em ordem de prioridade"""
        # Ordenar provedores por prioridade
        sorted_providers = sorted(self.providers.items(), key=lambda x: x[1].get('priority', 999))
        
        for provider_name, config in sorted_providers:
            if provider_name == 'ollama':
                # Verificar se Ollama est√° rodando
                if self._check_ollama_availability():
                    try:
                        client = OpenAI(
                            api_key='ollama',  # Ollama n√£o precisa de key real
                            base_url=config['base_url']
                        )
                        self.available_providers.append({
                            'name': provider_name,
                            'config': config,
                            'client': client,
                            'type': 'openai_compatible'
                        })
                        self.logger.info(f"‚úÖ Provedor {provider_name} configurado (local) - PRIORIDADE 1")
                    except Exception as e:
                        self.logger.warning(f"‚ùå Erro ao configurar {provider_name}: {e}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Ollama n√£o est√° dispon√≠vel - verifique se est√° rodando")
            elif config['api_key'] and config['api_key'] != 'your_api_key_here':
                try:
                    if provider_name == 'huggingface':
                        # Hugging Face usa requests diretamente
                        self.available_providers.append({
                            'name': provider_name,
                            'config': config,
                            'client': None,
                            'type': 'huggingface'
                        })
                    elif OpenAI:
                        # Outros provedores usam OpenAI client
                        client = OpenAI(
                            api_key=config['api_key'],
                            base_url=config['base_url']
                        )
                        self.available_providers.append({
                            'name': provider_name,
                            'config': config,
                            'client': client,
                            'type': 'openai_compatible'
                        })
                    
                    self.logger.info(f"‚úÖ Provedor {provider_name} configurado - PRIORIDADE {config['priority']}")
                except Exception as e:
                    self.logger.warning(f"‚ùå Erro ao configurar {provider_name}: {e}")
        
        if self.available_providers:
            primary_provider = self.available_providers[0]['name']
            self.logger.info(f"üîß {len(self.available_providers)} provedores dispon√≠veis")
            self.logger.info(f"üéØ Provedor principal: {primary_provider}")
            self.logger.info(f"üìã Ordem: {[p['name'] for p in self.available_providers]}")
        else:
            self.logger.warning("‚ö†Ô∏è Nenhum provedor de LLM configurado. Usando an√°lise local.")
    
    def analyze_news(self, news_item: Dict) -> Optional[Dict]:
        """Analisar uma not√≠cia usando sistema de fallback"""
        
        self.logger.info(f"üîç Iniciando an√°lise da not√≠cia: {news_item.get('title', 'N/A')[:50]}...")
        
        if not self.analysis_enabled:
            self.logger.info("üìù An√°lise desabilitada, usando an√°lise local")
            return self._local_analysis(news_item)
        
        # Tentar cada provedor dispon√≠vel
        for i, provider in enumerate(self.available_providers):
            try:
                self.logger.info(f"üöÄ Tentando provedor {provider['name']} ({i+1}/{len(self.available_providers)})")
                
                if provider['type'] == 'huggingface':
                    result = self._analyze_with_huggingface(news_item, provider)
                else:
                    result = self._analyze_with_openai_compatible(news_item, provider)
                
                if result:
                    self.logger.info(f"‚úÖ An√°lise conclu√≠da com {provider['name']}")
                    return result
                    
            except Exception as e:
                self.logger.warning(f"‚ùå Erro com {provider['name']}: {str(e)}")
                
                # Se for rate limit, tentar pr√≥ximo provedor
                if "429" in str(e) or "rate" in str(e).lower():
                    self.logger.info(f"‚è≥ Rate limit em {provider['name']}, tentando pr√≥ximo...")
                    continue
                else:
                    self.logger.error(f"üîç Erro t√©cnico em {provider['name']}: {type(e).__name__}")
        
        # Se todos os provedores falharam, usar an√°lise local
        self.logger.warning("‚ö†Ô∏è Todos os provedores falharam, usando an√°lise local")
        return self._local_analysis(news_item)
    
    def _analyze_with_openai_compatible(self, news_item: Dict, provider: Dict) -> Optional[Dict]:
        """Analisar usando provedor compat√≠vel com OpenAI"""
        
        # Primeiro, traduzir se necess√°rio (exceto para Ollama que pode ser mais lento)
        if provider['name'] != 'ollama':
            translated_item = self._translate_news_item_simple(news_item)
        else:
            translated_item = news_item
        
        # Criar prompt para an√°lise
        prompt = self._create_analysis_prompt(translated_item)
        
        # Configura√ß√µes espec√≠ficas para Ollama
        max_tokens = 1500 if provider['name'] == 'ollama' else 2000
        temperature = 0.5 if provider['name'] == 'ollama' else 0.7
        
        response = provider['client'].chat.completions.create(
            model=provider['config']['model'],
            messages=[
                {
                    "role": "system",
                    "content": "Voc√™ √© um analista especializado em intelig√™ncia artificial e tecnologia. Sua tarefa √© analisar not√≠cias sobre IA e fornecer insights espec√≠ficos para desenvolvedores de software e empresas do ramo imobili√°rio/shopping centers. Responda sempre em portugu√™s brasileiro."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        analysis_text = response.choices[0].message.content
        parsed_analysis = self._parse_analysis_response(analysis_text)
        
        # Adicionar t√≠tulo e resumo traduzidos
        parsed_analysis['titulo_traduzido'] = translated_item.get('title', news_item.get('title', ''))
        parsed_analysis['resumo_traduzido'] = translated_item.get('summary', news_item.get('summary', ''))
        parsed_analysis['provedor_usado'] = provider['name']  # Indicar qual provedor foi usado
        
        return parsed_analysis
    
    def _analyze_with_huggingface(self, news_item: Dict, provider: Dict) -> Optional[Dict]:
        """Analisar usando Hugging Face Inference API"""
        
        headers = {"Authorization": f"Bearer {provider['config']['api_key']}"}
        
        # Criar prompt simplificado para Hugging Face
        title = news_item.get('title', '')
        content = news_item.get('content', news_item.get('summary', ''))[:500]  # Limitar tamanho
        
        prompt = f"Analise esta not√≠cia de IA: {title}. {content}"
        
        api_url = f"{provider['config']['base_url']}/{provider['config']['model']}"
        
        response = requests.post(
            api_url,
            headers=headers,
            json={"inputs": prompt, "parameters": {"max_length": 500}},
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                analysis_text = result[0].get('generated_text', '')
                
                # Parse b√°sico para Hugging Face
                return {
                    'resumo_executivo': analysis_text[:200] + '...' if len(analysis_text) > 200 else analysis_text,
                    'pontos_chave': '‚Ä¢ An√°lise via Hugging Face\n‚Ä¢ Conte√∫do processado automaticamente\n‚Ä¢ Relevante para IA e tecnologia',
                    'nivel_relevancia': 'M√©dio - An√°lise autom√°tica via Hugging Face',
                    'titulo_traduzido': title,
                    'resumo_traduzido': news_item.get('summary', 'Resumo n√£o dispon√≠vel')
                }
        
        return None
    
    def _translate_news_item_simple(self, news_item: Dict) -> Dict:
        """Tradu√ß√£o simples ou manter original se n√£o conseguir traduzir"""
        
        title = news_item.get('title', '')
        summary = news_item.get('summary', news_item.get('content', ''))
        
        # Se j√° estiver em portugu√™s, n√£o traduzir
        if self._is_portuguese(title) and self._is_portuguese(summary):
            return news_item
        
        # Tentar traduzir apenas com o primeiro provedor dispon√≠vel
        if self.available_providers:
            try:
                provider = self.available_providers[0]
                if provider['type'] == 'openai_compatible':
                    
                    translation_prompt = f"""
Traduza para portugu√™s brasileiro:

T√çTULO: {title}
RESUMO: {summary}

Responda apenas:
T√çTULO_TRADUZIDO: [t√≠tulo em portugu√™s]
RESUMO_TRADUZIDO: [resumo em portugu√™s]
"""
                    
                    response = provider['client'].chat.completions.create(
                        model=provider['config']['model'],
                        messages=[
                            {"role": "system", "content": "Voc√™ √© um tradutor. Traduza para portugu√™s brasileiro."},
                            {"role": "user", "content": translation_prompt}
                        ],
                        max_tokens=1000,
                        temperature=0.3
                    )
                    
                    translation_text = response.choices[0].message.content
                    
                    # Parse da tradu√ß√£o
                    translated_title = title
                    translated_summary = summary
                    
                    lines = translation_text.split('\n')
                    for line in lines:
                        if line.startswith('T√çTULO_TRADUZIDO:'):
                            translated_title = line.replace('T√çTULO_TRADUZIDO:', '').strip()
                        elif line.startswith('RESUMO_TRADUZIDO:'):
                            translated_summary = line.replace('RESUMO_TRADUZIDO:', '').strip()
                    
                    translated_item = news_item.copy()
                    translated_item['title'] = translated_title
                    translated_item['summary'] = translated_summary
                    
                    return translated_item
                    
            except Exception as e:
                self.logger.warning(f"‚ùå Erro na tradu√ß√£o: {e}")
        
        return news_item
    
    def _is_portuguese(self, text: str) -> bool:
        """Verificar se o texto est√° em portugu√™s (verifica√ß√£o simples)"""
        portuguese_words = ['de', 'da', 'do', 'para', 'com', 'em', 'por', 'uma', 'um', 'que', 'n√£o', 's√£o', 'como', 'mais', 'sobre', 'pela', 'pelo']
        words = text.lower().split()
        portuguese_count = sum(1 for word in words if word in portuguese_words)
        return portuguese_count >= 2 or len(words) < 5
    
    def _create_analysis_prompt(self, news_item: Dict) -> str:
        """Criar prompt para an√°lise da not√≠cia"""
        
        title = news_item.get('title', 'N/A')
        content = news_item.get('content', news_item.get('summary', 'N/A'))
        
        prompt = f"""
Analise a seguinte not√≠cia sobre intelig√™ncia artificial e tecnologia:

**T√≠tulo:** {title}

**Conte√∫do:** {content}

Por favor, forne√ßa uma an√°lise estruturada seguindo EXATAMENTE este formato (mantenha os cabe√ßalhos):

## RESUMO EXECUTIVO
[Resumo da not√≠cia, em portugu√™s brasileiro]

## PONTOS-CHAVE
[3-5 pontos principais desta not√≠cia em formato de lista]

## N√çVEL DE RELEV√ÇNCIA
[Alto/M√©dio/Baixo] - [Justificativa em uma frase]

IMPORTANTE: Responda sempre em portugu√™s brasileiro e mantenha exatamente os cabe√ßalhos mostrados acima.
"""
        
        return prompt
    
    def _parse_analysis_response(self, analysis_text: str) -> Dict:
        """Fazer parse da resposta da an√°lise"""
        
        self.logger.info("üìã Fazendo parse da resposta da an√°lise...")
        
        sections = {
            'resumo_executivo': '',
            'pontos_chave': '',
            'nivel_relevancia': ''
        }
        
        current_section = None
        lines = analysis_text.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # Detectar se√ß√µes com ## ou **
            if ('## RESUMO EXECUTIVO' in line.upper() or 
                '**RESUMO EXECUTIVO**' in line.upper() or
                'RESUMO EXECUTIVO' in line.upper()):
                current_section = 'resumo_executivo'
                self.logger.debug("üìù Encontrada se√ß√£o: RESUMO EXECUTIVO")
            elif ('## PONTOS-CHAVE' in line.upper() or '## PONTOS CHAVE' in line.upper() or
                  '**PONTOS-CHAVE**' in line.upper() or '**PONTOS CHAVE**' in line.upper() or
                  'PONTOS-CHAVE' in line.upper() or 'PONTOS CHAVE' in line.upper()):
                current_section = 'pontos_chave'
                self.logger.debug("üìù Encontrada se√ß√£o: PONTOS-CHAVE")
            elif ('## N√çVEL DE RELEV√ÇNCIA' in line.upper() or '## NIVEL DE RELEVANCIA' in line.upper() or
                  '**N√çVEL DE RELEV√ÇNCIA**' in line.upper() or '**NIVEL DE RELEVANCIA**' in line.upper() or
                  'N√çVEL DE RELEV√ÇNCIA' in line.upper() or 'NIVEL DE RELEVANCIA' in line.upper()):
                current_section = 'nivel_relevancia'
                self.logger.debug("üìù Encontrada se√ß√£o: N√çVEL DE RELEV√ÇNCIA")
            elif current_section and line and not line.startswith('##') and not line.startswith('**'):
                # Pular linhas que s√£o apenas marcadores de se√ß√£o
                if not any(marker in line.upper() for marker in ['RESUMO EXECUTIVO', 'PONTOS-CHAVE', 'PONTOS CHAVE', 'N√çVEL DE RELEV√ÇNCIA', 'NIVEL DE RELEVANCIA']):
                    sections[current_section] += line + '\n'
        
        # Limpar se√ß√µes
        for key in sections:
            sections[key] = sections[key].strip()
        
        return sections
    
    def _local_analysis(self, news_item: Dict) -> Dict:
        """An√°lise local inteligente baseada em palavras-chave e padr√µes"""
        
        title = news_item.get('title', 'N/A')
        content = news_item.get('content', news_item.get('summary', ''))
        
        self.logger.info("üè† Gerando an√°lise local inteligente")
        
        # An√°lise de relev√¢ncia baseada em palavras-chave
        high_relevance_keywords = ['openai', 'chatgpt', 'claude', 'gemini', 'breakthrough', 'revolutionary', 'billion', 'funding', 'ipo', 'acquisition']
        medium_relevance_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural', 'algorithm', 'automation', 'robot']
        low_relevance_keywords = ['update', 'minor', 'patch', 'bug', 'fix']
        
        text_lower = (title + ' ' + content).lower()
        
        # Calcular relev√¢ncia
        high_count = sum(1 for keyword in high_relevance_keywords if keyword in text_lower)
        medium_count = sum(1 for keyword in medium_relevance_keywords if keyword in text_lower)
        low_count = sum(1 for keyword in low_relevance_keywords if keyword in text_lower)
        
        if high_count >= 2:
            relevancia = 'Alto - Cont√©m m√∫ltiplas palavras-chave de alta relev√¢ncia'
        elif high_count >= 1 or medium_count >= 3:
            relevancia = 'M√©dio - Relev√¢ncia moderada para o setor de IA'
        elif medium_count >= 1:
            relevancia = 'M√©dio - Relacionado √† intelig√™ncia artificial'
        else:
            relevancia = 'Baixo - Relev√¢ncia limitada para IA'
        
        # Gerar pontos-chave baseados no conte√∫do
        pontos_chave = []
        if 'openai' in text_lower or 'chatgpt' in text_lower:
            pontos_chave.append('‚Ä¢ Desenvolvimento em OpenAI/ChatGPT')
        if 'google' in text_lower or 'gemini' in text_lower:
            pontos_chave.append('‚Ä¢ Inova√ß√£o do Google em IA')
        if 'microsoft' in text_lower:
            pontos_chave.append('‚Ä¢ Iniciativa da Microsoft')
        if 'funding' in text_lower or 'investment' in text_lower:
            pontos_chave.append('‚Ä¢ Movimenta√ß√£o de investimentos')
        if 'enterprise' in text_lower or 'business' in text_lower:
            pontos_chave.append('‚Ä¢ Aplica√ß√£o empresarial')
        if 'developer' in text_lower or 'api' in text_lower:
            pontos_chave.append('‚Ä¢ Ferramentas para desenvolvedores')
        
        if not pontos_chave:
            pontos_chave = [
                '‚Ä¢ Not√≠cia relacionada √† intelig√™ncia artificial',
                '‚Ä¢ Potencial impacto no setor de tecnologia',
                '‚Ä¢ Relevante para acompanhamento do mercado'
            ]
        
        # Resumo executivo inteligente
        resumo = f"An√°lise local da not√≠cia: {title}. "
        if high_count > 0:
            resumo += "Esta not√≠cia apresenta desenvolvimentos significativos em IA com potencial impacto no mercado. "
        elif medium_count > 0:
            resumo += "Esta not√≠cia aborda temas relevantes de intelig√™ncia artificial. "
        else:
            resumo += "Esta not√≠cia tem conex√£o com tecnologia e pode ser de interesse. "
        
        resumo += "Recomenda-se acompanhamento para avaliar oportunidades de aplica√ß√£o em neg√≥cios e desenvolvimento."
        
        return {
            'resumo_executivo': resumo,
            'pontos_chave': '\n'.join(pontos_chave),
            'nivel_relevancia': relevancia,
            'titulo_traduzido': title,
            'resumo_traduzido': news_item.get('summary', 'Resumo n√£o dispon√≠vel'),
            'metodo_analise': 'local'  # Indicador de que foi an√°lise local
        }