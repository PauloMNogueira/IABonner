# ğŸ¤– AI News Agent - JCPM

Um agente inteligente para fazer crawler e anÃ¡lise de notÃ­cias sobre InteligÃªncia Artificial dos principais sites especializados, com foco em insights para desenvolvedores de software e empresas do ramo imobiliÃ¡rio/shopping centers.

## ğŸ¯ Funcionalidades

- **Crawler Automatizado**: Coleta notÃ­cias de mÃºltiplas fontes especializadas em IA
- **AnÃ¡lise Inteligente**: Usa LLM via OpenRouter para analisar cada notÃ­cia
- **Insights Personalizados**: Gera anÃ¡lises especÃ­ficas para:
  - Desenvolvedores de software
  - Empresa JCPM (shopping centers/imobiliÃ¡rio)
- **ExecuÃ§Ã£o Agendada**: Roda automaticamente 2x por dia
- **GitHub Pages**: Gera site HTML automaticamente hospedado
- **MÃºltiplos Formatos**: Salva dados em TXT e JSON

## ğŸ“ Estrutura do Projeto

```
agent_ai_news/
â”œâ”€â”€ sources/                    # Crawlers por fonte
â”‚   â”œâ”€â”€ base_crawler.py        # Classe base
â”‚   â”œâ”€â”€ techcrunch_crawler.py  # TechCrunch
â”‚   â”œâ”€â”€ venturebeat_crawler.py # VentureBeat
â”‚   â””â”€â”€ mit_technology_review_crawler.py # MIT Tech Review
â”œâ”€â”€ src/                       # CÃ³digo principal
â”‚   â”œâ”€â”€ crawler_manager.py     # Gerenciador de crawlers
â”‚   â”œâ”€â”€ news_analyzer.py       # AnÃ¡lise com LLM
â”‚   â”œâ”€â”€ html_generator.py      # Gerador de HTML
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py          # Sistema de logging
â”œâ”€â”€ docs/                      # HTML para GitHub Pages
â”œâ”€â”€ news_data/                 # Dados coletados
â”œâ”€â”€ logs/                      # Arquivos de log
â”œâ”€â”€ .github/workflows/         # GitHub Actions
â”œâ”€â”€ main.py                    # Script principal
â”œâ”€â”€ scheduler.py               # Agendador
â”œâ”€â”€ run_once.py               # ExecuÃ§Ã£o Ãºnica
â””â”€â”€ requirements.txt          # DependÃªncias
```

## ğŸš€ ConfiguraÃ§Ã£o

### 1. Clonar o RepositÃ³rio

```bash
git clone <seu-repositorio>
cd agent_ai_news
```

### 2. Instalar DependÃªncias

```bash
pip install -r requirements.txt
```

### 3. Configurar VariÃ¡veis de Ambiente

Copie o arquivo de exemplo e configure suas chaves:

```bash
cp .env.example .env
```

Edite o arquivo `.env`:

```env
# OpenRouter API Configuration
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Model Configuration
DEFAULT_MODEL=anthropic/claude-3-haiku

# GitHub Configuration (para GitHub Pages)
GITHUB_TOKEN=your_github_token_here
GITHUB_REPO=your_username/your_repo_name
GITHUB_BRANCH=gh-pages

# News Configuration
MAX_NEWS_PER_SOURCE=10
OUTPUT_DIR=news_data
HTML_OUTPUT_DIR=docs
```

### 4. Obter Chave da OpenRouter

1. Acesse [OpenRouter](https://openrouter.ai/)
2. Crie uma conta
3. Gere uma API key
4. Adicione a chave no arquivo `.env`

## ğŸ® Como Usar

### ExecuÃ§Ã£o Ãšnica (Teste)

```bash
python run_once.py
```

### ExecuÃ§Ã£o com Agendamento

```bash
python scheduler.py
```

### ExecuÃ§Ã£o Manual

```bash
python main.py
```

## ğŸ”„ AutomaÃ§Ã£o com GitHub Actions

O projeto inclui automaÃ§Ã£o completa via GitHub Actions:

### ConfiguraÃ§Ã£o no GitHub

1. **Secrets necessÃ¡rios**:
   - `OPENROUTER_API_KEY`: Sua chave da OpenRouter

2. **Habilitar GitHub Pages**:
   - VÃ¡ em Settings > Pages
   - Source: Deploy from a branch
   - Branch: `gh-pages`

### Funcionamento

- **ExecuÃ§Ã£o automÃ¡tica**: 2x por dia (08:00 e 18:00 UTC)
- **ExecuÃ§Ã£o manual**: Via GitHub Actions tab
- **Deploy automÃ¡tico**: Para GitHub Pages apÃ³s cada execuÃ§Ã£o

## ğŸ“Š Fontes de NotÃ­cias

Atualmente o sistema coleta de:

- **TechCrunch** - SeÃ§Ã£o de IA
- **VentureBeat** - Feed RSS de IA  
- **MIT Technology Review** - SeÃ§Ã£o de IA

### Adicionar Nova Fonte

1. Crie um novo crawler em `sources/`:

```python
from .base_crawler import BaseCrawler

class NovaFonteCrawler(BaseCrawler):
    def __init__(self):
        super().__init__("Nova Fonte", "https://exemplo.com")
    
    def get_news_urls(self):
        # Implementar lÃ³gica para obter URLs
        pass
    
    def extract_article_content(self, url):
        # Implementar extraÃ§Ã£o de conteÃºdo
        pass
```

2. Adicione no `crawler_manager.py`:

```python
from sources.nova_fonte_crawler import NovaFonteCrawler

# No __init__:
self.crawlers = [
    TechCrunchCrawler(),
    VentureBeatCrawler(),
    MITTechnologyReviewCrawler(),
    NovaFonteCrawler()  # Adicionar aqui
]
```

## ğŸ“ˆ AnÃ¡lise com IA

Cada notÃ­cia Ã© analisada pelo LLM gerando:

### ğŸ“‹ Resumo Executivo
Resumo conciso da notÃ­cia

### ğŸ‘¨â€ğŸ’» Impacto para Desenvolvedores
- Novas oportunidades de desenvolvimento
- Tecnologias que podem ser adotadas
- MudanÃ§as nas prÃ¡ticas de desenvolvimento
- Impacto nas ferramentas e frameworks

### ğŸ¢ Impacto para JCPM (Shopping Centers/ImobiliÃ¡rio)
- Oportunidades de implementaÃ§Ã£o de IA
- Melhorias na experiÃªncia do cliente
- OtimizaÃ§Ã£o de operaÃ§Ãµes
- Novas fontes de receita
- AutomaÃ§Ã£o de processos

### ğŸ¯ Pontos-Chave
Lista dos principais pontos da notÃ­cia

### â­ NÃ­vel de RelevÃ¢ncia
ClassificaÃ§Ã£o: Alto/MÃ©dio/Baixo com justificativa

## ğŸ“± Interface Web

O sistema gera automaticamente um site HTML moderno com:

- **Design Responsivo**: Funciona em desktop e mobile
- **Filtros**: Por nÃ­vel de relevÃ¢ncia
- **AnimaÃ§Ãµes**: Interface fluida e moderna
- **API JSON**: Dados disponÃ­veis em formato JSON

### Acesso ao Site

ApÃ³s configurar GitHub Pages: `https://seu-usuario.github.io/agent_ai_news`

## ğŸ“ Logs e Monitoramento

- **Logs detalhados**: Salvos em `logs/`
- **Formato estruturado**: Data, hora, nÃ­vel, mensagem
- **RotaÃ§Ã£o automÃ¡tica**: Por data
- **Console e arquivo**: SaÃ­da dupla

## ğŸ› ï¸ Desenvolvimento

### Estrutura de Classes

```python
# Crawler base
BaseCrawler
â”œâ”€â”€ get_news_urls()          # Obter URLs das notÃ­cias
â”œâ”€â”€ extract_article_content() # Extrair conteÃºdo
â””â”€â”€ crawl()                  # Processo completo

# Gerenciador
CrawlerManager
â”œâ”€â”€ crawl_all_sources()      # Executar todos os crawlers
â”œâ”€â”€ save_news_to_file()      # Salvar em TXT
â””â”€â”€ save_consolidated_news() # Salvar JSON consolidado

# Analisador
NewsAnalyzer
â”œâ”€â”€ analyze_news()           # Analisar com LLM
â”œâ”€â”€ _create_analysis_prompt() # Criar prompt
â””â”€â”€ _parse_analysis_response() # Parse da resposta

# Gerador HTML
HTMLGenerator
â”œâ”€â”€ generate_html()          # Gerar site completo
â”œâ”€â”€ _generate_index_page()   # PÃ¡gina principal
â”œâ”€â”€ _generate_css()          # Estilos
â””â”€â”€ _generate_js()           # JavaScript
```

### Testes

```bash
# Testar crawler especÃ­fico
python -c "from sources.techcrunch_crawler import TechCrunchCrawler; c = TechCrunchCrawler(); print(c.crawl(max_articles=2))"

# Testar anÃ¡lise
python -c "from src.news_analyzer import NewsAnalyzer; a = NewsAnalyzer(); print(a._simulate_analysis({'title': 'Test'}))"
```

## ğŸ”§ Troubleshooting

### Problemas Comuns

1. **Erro de API Key**:
   - Verifique se `OPENROUTER_API_KEY` estÃ¡ configurada
   - Teste a chave no site da OpenRouter

2. **Crawler nÃ£o encontra notÃ­cias**:
   - Sites podem ter mudado estrutura HTML
   - Verifique logs para detalhes
   - Atualize seletores CSS se necessÃ¡rio

3. **GitHub Pages nÃ£o atualiza**:
   - Verifique se GitHub Actions estÃ¡ habilitado
   - Confirme se branch `gh-pages` existe
   - Verifique permissÃµes do repositÃ³rio

4. **DependÃªncias**:
   ```bash
   pip install --upgrade -r requirements.txt
   ```

### Debug

```bash
# Executar com logs detalhados
python -c "
import logging
logging.basicConfig(level=logging.DEBUG)
from main import main
main()
"
```

## ğŸ“„ LicenÃ§a

Este projeto Ã© desenvolvido para uso interno da JCPM.

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

---

**Desenvolvido com â¤ï¸ e IA para JCPM**